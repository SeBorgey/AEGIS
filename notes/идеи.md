### Заметки:

PySide6  предпочитать Widgets вместо QML
PyInstaller
проверки ruff и mypy

Тестирование ядра и офлайн‑оценка
Набор эталонных ТЗ (50–200 штук) с ожидаемыми результатами:
Примеры: конвертер CSV→XLSX с GUI; менеджер заметок (sqlite); загрузчик изображений; простой HTTP‑клиент; просмотрщик PDF; таймер/помодоро; визуализатор данных (matplotlib).



FastAPI + очередь + статус‑машина задач.
Сделать это отдельным контейнером, а обращаться  к нему по API



Синтаксическая проверка (ast.parse)
Проверка импортов
Pylint/Flake8/Black для качества кода
Проверка типов (mypy)
Проверка безопасности (bandit)


Запуск приложения в headless режиме (QT_QPA_PLATFORM=offscreen) - консольный тестовый запуск без изображения



Определить Pydantic‑схемы ActionCall/ActionResult, завести ActionRegistry

докер только для запуска кода



### План доработок:
- Хотелось бы сделать отдельный фреймворк из того (Action API), что модель может совершать на компьютере. Сделать pydantic модель для этих целей как абстрактный класс.
- Добавить uv.
- Добавить планирование.
- Выполнять код в докере.
- Подробная система логирования. Целый модуль. Выводить в лог не только всё, что происходило в программе, но и записывать полную историю чата с моделью.

Пример Action API:
```
from pydantic import BaseModel, Field
from typing import Any, Dict, Optional

class ActionCall(BaseModel):
    name: str
    params: Dict[str, Any] = Field(default_factory=dict)

class ActionResult(BaseModel):
    success: bool
    data: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    logs: str = ""
    duration_ms: int = 0

class ActionExecutor:
    def __init__(self, policy, registry):
        self.policy = policy
        self.registry = registry  # name -> callable

    def execute(self, call: ActionCall) -> ActionResult:
        self.policy.check(call)  # пути, команды, таймауты
        fn = self.registry.get(call.name)
        if not fn:
            return ActionResult(success=False, error=f"Unknown action {call.name}")
        return fn(**call.params)
```

### Список тулов:
- прочитать файл
- создать файл
- выполнить команду в терминале
- отредактировать файл (заменить старый текст файла новым)
- запустить тесты? (вообще-то можно и автоматически)


Сделать чекер, который будет проверять, насколько код соответствует ТЗ. Его вердикт можно парсить классическим ML. Чтобы потом по результатам парсинга отправлять или не отправлять задание на доработку.
Сделать расширитель ТЗ: если ТЗ написано двумя словами, отдельный промпт будет призван его расширить, чтобы превзойти ожидания пользователя.
Еще один этап нужен для составления плана - планировщик.


## Задачи
### Оценка агента
- проверить бенчмарки агентов можно ли на них скориться
- узнать есть ли бенчи на написание программ
- взять их себе
- реализовать скоринг на бенчах

### Тулы
- поискать статьи про тулы
- выбрать sota набор тулов
- добавить их в свой код

### Генерация датасета для автоинтента
- почитать статью про евристики создания датасетов
- поискать еще такие статьи
- создать сборщик датасета - при каждом запуске добавлять новую семпл в датасет



## Решения
### Оценка агента
- https://github.com/open-compass/DevEval и devbench статья там есть список программ. Из него сделан hard датасет.
- Чтобы норм бенчмаркать нужен еще один агент который кликает по приложению. Запускать в эмуляторе, отправлять ему скриншоты. Этого агента на самом деле можно использовать как тестировщика на постоянке.
- Есть куча статей про то, как это реализовать:
AppAgent
Mind2Web
WebArena
OSWORLD
AppWorld
MOBILE-AGENT
- SWE-bench не подходит для обоих целей. Он проверяет как LLM умеет решать проблемы в репозиториях.

### Тулы
Coding Agents - исследования показали что нужен терминал питона и поиск по документации, навигация по репе.
HyperAgent - статья с конкретными тулами для агента.

Итого добавятся тулы:
get_file_tree (HyperAgent) - дерево с ограниченной глубиной
run_ipython (Coding Agents) - интерактвная среда выполнения питона
finish_task (HyperAgent) - уже есть, но сделать из этого тул, который будет запускать приложение и тестировать.

Интересное
Agentless - LLM, ограниченная жестким алгоритмом работает лучше.
HyperAgent исследования показали, что лучше когда у агента 4 тула. Поэтому они сделали несколько и каждому раздали свои тулы, качество выросло.

### Генерация датасета для автоинтента
? Мы будем делать это на английском или на русском?
на английском!
https://huggingface.co/spaces/mteb/leaderboard - бенчмарк моделей для перевода текста в ембединги.
Выбора нет, только Qwen/Qwen3-Embedding-0.6B

Эвристики генерации на основе core-set-survey:
1) не добавляем примеры, которые привели к провалу (с.11-12)
2) Сохранять только те, которые после преобразования в эмбединг отличаются сильно от тех, что уже были в базе (с.3-4 Geometry Based, формула 7). На самом деле просто жадный алгоритм. Из набранных векторов набираем k штук. Каждый раз берем наиболее отдалённый от тех, что уже взяли.
3) Если датасет большой, то нужны сложные примеры, чтобы лучше разграничить классы, для маленьких наоборот легкие (с.11). Можем по длительности мыслей определить сложность примера и выдать ему бинарный признак сложности (с.4)
4) Набираем так, чтобы классы были как можно более сбалансированными. То есть если у нас есть 1000 на edit и 10 на get_tree, то набираем дальше только get_tree. (с.6)

То есть на этапе набора используем только пункт 1 и 3. На этапе отбора умный алгоритм выделит нам датасет по 2 и 4 - это всё может делать один жадный алгоритм (с.6 диаграмма).